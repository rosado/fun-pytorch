{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799d9c0e-05ad-44c1-9236-15053d5e5b94",
   "metadata": {},
   "source": [
    "# scikit-learn scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2d8fba-616e-47fd-aa35-563aee109616",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea95ea98-5e10-4749-88a7-e10615f5b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf4e75cb-9d5b-4c5c-b5b2-5818f7b0f6df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608 2421\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Rect:\n",
    "    \"\"\"Specifies the region of the image we're interested in. \n",
    "    Resolution is a width:height of the image.\"\"\"\n",
    "    top: float\n",
    "    left: float\n",
    "    width: float\n",
    "    height: float\n",
    "    resolution: tuple = (2560, 1440)\n",
    "\n",
    "    @property\n",
    "    def right(self):\n",
    "        return self.left + self.resolution[0] - self.width\n",
    "\n",
    "    @property\n",
    "    def bottom(self):\n",
    "        return self.top + self.resolution[1] - self.height\n",
    "\n",
    "if EXAMPLES:\n",
    "    r = Rect(left=165, top=1192, width=2117, height=211)\n",
    "    print(r.right, r.bottom)\n",
    "    r = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d316a5f-5b3e-4efa-8a59-f46026d6f03f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Takes care of loading images from disk & transforming them.\n",
    "    Makes use of predefined classes: 'other' and 'starmap'.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, crop_rect: Rect, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['other', 'starmap']\n",
    "        self.file_paths = []\n",
    "        self.crop_rect = crop_rect\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for file in os.listdir(class_path):\n",
    "                self.file_paths.append(\n",
    "                    (os.path.join(class_path, file), self.classes.index(class_name))\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        rect = self.crop_rect\n",
    "        image = image.crop((rect.left, rect.top, rect.right, rect.bottom))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab477c56-2b33-44f1-85bd-211ec31158eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Session:\n",
    "    transform: torchvision.transforms.transforms.Compose\n",
    "    crop_region: Rect\n",
    "    optimizer: object\n",
    "    criterion: object\n",
    "    device: object\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    dataset: CustomDataset\n",
    "    batch_size: int\n",
    "    shuffle: bool\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        return int(0.8 * len(self.dataset))\n",
    "\n",
    "    @property\n",
    "    def val_size(self):\n",
    "        return len(self.dataset) - self.train_size\n",
    "\n",
    "    def dataset_split(self):\n",
    "         return torch.utils.data.random_split(self.dataset, [self.train_size, self.val_size])\n",
    "\n",
    "@dataclass\n",
    "class TrainingResult:\n",
    "    model: object\n",
    "    loader_val: object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9b6abb-df7e-4583-804b-252fd5302ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.3512\n",
      "Epoch 2/10 - Loss: 0.0515\n",
      "Epoch 3/10 - Loss: 0.0263\n",
      "Epoch 4/10 - Loss: 0.0018\n",
      "Epoch 5/10 - Loss: 0.0013\n",
      "Epoch 6/10 - Loss: 0.0024\n",
      "Epoch 7/10 - Loss: 0.0005\n",
      "Epoch 8/10 - Loss: 0.0014\n",
      "Epoch 9/10 - Loss: 0.0006\n",
      "Epoch 10/10 - Loss: 0.0013\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       1.00      1.00      1.00        11\n",
      "     starmap       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CROP_REGION = Rect(left=165, top=1192, width=2117, height=211)\n",
    "\n",
    "def make_criterion(): return nn.CrossEntropyLoss()\n",
    "def make_optimizer(model): return optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "sess = Session(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    crop_region=CROP_REGION,\n",
    "    criterion=make_criterion,\n",
    "    optimizer=make_optimizer,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "data = Data(\n",
    "    dataset=CustomDataset('data/screenshots', transform=sess.transform, crop_rect=CROP_REGION),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "def train(sess: Session, data: Data, model, num_epochs=5):\n",
    "    dataset_train, dataset_val = data.dataset_split()\n",
    "    loader_train = DataLoader(dataset_train, batch_size=data.batch_size, shuffle=data.shuffle)\n",
    "    loader_val = DataLoader(dataset_val, batch_size=data.batch_size, shuffle=data.shuffle)\n",
    "\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2) # 2 output classes\n",
    "    device = sess.device\n",
    "    device_model = model.to(device)\n",
    "\n",
    "    optimizer = sess.optimizer(device_model)\n",
    "    criterion = sess.criterion()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        device_model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels, in loader_train:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = device_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset_train)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    return TrainingResult(model=device_model, loader_val=loader_val)\n",
    "\n",
    "def evaluate(result: TrainingResult, device):\n",
    "    model, val_loader = result.model, result.loader_val\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=data.dataset.classes))\n",
    "    \n",
    "\n",
    "if EXAMPLES:\n",
    "    model = models.resnet34()\n",
    "    result = train(sess, data, model, num_epochs=10)\n",
    "    evaluate(result, sess.device)\n",
    "\n",
    "# Evaluation\n",
    "# if False:\n",
    "#     model.eval()\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.numpy())\n",
    "\n",
    "#     print(classification_report(all_labels, all_preds, target_names=data.dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cb8e93-ccb0-4ee7-82a5-f18d833f521b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68475e21-926f-4199-a7e4-143d8d6da343",
   "metadata": {},
   "source": [
    "let's save the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ebe2267-b2b0-4ddc-a476-1feeeb88e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet34_state_dict_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba290d8-b600-4ba7-8a22-95f61c5ace97",
   "metadata": {},
   "source": [
    "Let's try to use the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06815611-3a80-4062-bb38-1595b1881bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    # Initialize model\n",
    "    model = models.resnet34()\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device,weights_only=False))\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# Preprocessing (must match training preprocessing)\n",
    "def preprocess_image(image_path):    \n",
    "    img = Image.open(image_path).convert('RGB')    \n",
    "    r = CROP_REGION\n",
    "    img = img.crop((r.left, r.top, r.right, r.bottom)) \n",
    "    return transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Example usage\n",
    "def example_usage(image_path):\n",
    "    model = load_model('resnet34_state_dict_v2.pth')\n",
    "    input_tensor = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "        print(f'Predicted class: {[\"other\", \"starmap\"][predicted_class]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ca8079c-7436-4f72-8241-d6bd7739ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage('E:/bin/StarCitizen/LIVE/ScreenShots/ScreenShot-2024-05-11_21-58-40-1A3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfc49d14-3ca2-45c7-86c6-499c06be0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage('E:/bin/StarCitizen/LIVE/ScreenShots/ScreenShot-2024-05-12_00-09-07-EF4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ec5633a-a43e-45d3-b772-41feaaca9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2024-07-31_01-13-41-B9A.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e80d363a-c9b1-43b5-a255-077b34394de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2024-08-13_23-34-21-1CC.jpg\") # starmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85915bdb-e921-418b-b132-2f5b87dc644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2025-03-06_22-49-40-3A1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47a1647d-6e51-45b6-9f4a-8b7a66b52ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2025-03-06_23-52-52-0D2.jpg\") # starmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7678bc-6277-474b-822a-f4205b642a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
