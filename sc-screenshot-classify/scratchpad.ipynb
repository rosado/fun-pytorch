{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799d9c0e-05ad-44c1-9236-15053d5e5b94",
   "metadata": {},
   "source": [
    "# scikit-learn scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2d8fba-616e-47fd-aa35-563aee109616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefcca19-b277-4d7e-a02b-051b8ce20fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.0+cu124'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a9b6abb-df7e-4583-804b-252fd5302ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roland\\dev\\fun-pytorch\\sc-screenshot-classify\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.5795\n",
      "Epoch 2/10 - Loss: 1.0712\n",
      "Epoch 3/10 - Loss: 0.0215\n",
      "Epoch 4/10 - Loss: 0.0107\n",
      "Epoch 5/10 - Loss: 0.0020\n",
      "Epoch 6/10 - Loss: 0.0019\n",
      "Epoch 7/10 - Loss: 0.0052\n",
      "Epoch 8/10 - Loss: 0.0051\n",
      "Epoch 9/10 - Loss: 0.0015\n",
      "Epoch 10/10 - Loss: 0.0022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       other       1.00      1.00      1.00        11\n",
      "     starmap       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           1.00        15\n",
      "   macro avg       1.00      1.00      1.00        15\n",
      "weighted avg       1.00      1.00      1.00        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class with preprocessing\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['other', 'starmap']\n",
    "        self.file_paths = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            for file in os.listdir(class_path):\n",
    "                self.file_paths.append(\n",
    "                    (os.path.join(class_path, file), self.classes.index(class_name))\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.file_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Custom crop coordinates (adjust these values as needed)\n",
    "        left = 165   # starting x coordinate\n",
    "        top = 1192    # starting y coordinate\n",
    "        right = left + (2560 - 2117)  # ending x coordinate\n",
    "        bottom = top + (1440 - 211) # ending y coordinate\n",
    "        image = image.crop((left, top, right, bottom))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset and split\n",
    "dataset = CustomDataset('data/screenshots', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "model = models.resnet34(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 output classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=dataset.classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68475e21-926f-4199-a7e4-143d8d6da343",
   "metadata": {},
   "source": [
    "let's save the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ebe2267-b2b0-4ddc-a476-1feeeb88e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet34_state_dict_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba290d8-b600-4ba7-8a22-95f61c5ace97",
   "metadata": {},
   "source": [
    "Let's try to use the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06815611-3a80-4062-bb38-1595b1881bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_model(model_path, device='cpu'):\n",
    "    # Initialize model\n",
    "    model = models.resnet34(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device,weights_only=False))\n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# Preprocessing (must match training preprocessing)\n",
    "def preprocess_image(image_path):\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # ])\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    # Apply the same crop as during training\n",
    "    \n",
    "    \n",
    "    left = 165\n",
    "    top = 1192\n",
    "    right = left + (2560 - 2117) \n",
    "    bottom = top + (1440 - 211)\n",
    "    img = img.crop((left, top, right, bottom)) \n",
    "    return transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Example usage\n",
    "def example_usage(image_path):\n",
    "    model = load_model('resnet34_state_dict_v2.pth')\n",
    "    input_tensor = preprocess_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "        print(f'Predicted class: {[\"other\", \"starmap\"][predicted_class]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ca8079c-7436-4f72-8241-d6bd7739ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage('E:/bin/StarCitizen/LIVE/ScreenShots/ScreenShot-2024-05-11_21-58-40-1A3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfc49d14-3ca2-45c7-86c6-499c06be0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage('E:/bin/StarCitizen/LIVE/ScreenShots/ScreenShot-2024-05-12_00-09-07-EF4.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ec5633a-a43e-45d3-b772-41feaaca9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: other\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2024-07-31_01-13-41-B9A.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e80d363a-c9b1-43b5-a255-077b34394de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2024-08-13_23-34-21-1CC.jpg\") # starmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85915bdb-e921-418b-b132-2f5b87dc644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2025-03-06_22-49-40-3A1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47a1647d-6e51-45b6-9f4a-8b7a66b52ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: starmap\n"
     ]
    }
   ],
   "source": [
    "example_usage(r\"E:\\bin\\StarCitizen\\LIVE\\ScreenShots\\ScreenShot-2025-03-06_23-52-52-0D2.jpg\") # starmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7678bc-6277-474b-822a-f4205b642a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
